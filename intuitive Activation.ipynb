{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1146f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from dongnet import dongnet12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f7d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not abailable. Training on CPU...\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not abailable. Training on CPU...')\n",
    "else:\n",
    "    print('CUIDA is available! Training on GPU...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c0c56",
   "metadata": {},
   "source": [
    "# evaluate 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6142d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, NBIT):\n",
    "    #batch norm 고정, dropout 안함, gradient 계산안함\n",
    "    model.eval()\n",
    "    #model 파라미터를 지정한 device 메모리에 올림\n",
    "    model.to(device)\n",
    "\n",
    "    running_corrects = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #텐서의 최대값과 index를 반환\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # inputs.size(0) 현재 mini-batch의 크기(input의 0번째 dimension 크기)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    print(\"{}bit model cifar100 Accuracy : {:.4f}\".format(NBIT, eval_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6a283",
   "metadata": {},
   "source": [
    "# layer에 quantization 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38507b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dongnet12q(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(dongnet12q, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(negative_slope=0.1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.leaky_relu3 = nn.LeakyReLU(negative_slope=0.1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(128)\n",
    "        self.leaky_relu4 = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(128)\n",
    "        self.leaky_relu5 = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm6 = nn.BatchNorm2d(256)\n",
    "        self.leaky_relu6 = nn.LeakyReLU(negative_slope=0.1)   \n",
    "\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm7 = nn.BatchNorm2d(256)\n",
    "        self.leaky_relu7 = nn.LeakyReLU(negative_slope=0.1)   \n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm8 = nn.BatchNorm2d(256)\n",
    "        self.leaky_relu8 = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm9 = nn.BatchNorm2d(256)\n",
    "        self.leaky_relu9 = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        #self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.max_pool2d = nn.MaxPool2d(2, stride=2)\n",
    "        self.linear_relu = nn.Sequential(\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 100)\n",
    "        )\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        #self.relu = nn.ReLU()\n",
    "    \n",
    "    def quantize(self, X, NBIT=8):\n",
    "        # 1. find threshold\n",
    "        alpha = torch.max(X)\n",
    "        beta = torch.min(X)\n",
    "        alpha_q = -2**(NBIT - 1)\n",
    "        beta_q = 2**(NBIT - 1) - 1\n",
    "\n",
    "        s = (beta - alpha) / (beta_q - alpha_q)\n",
    "        z = int((beta*alpha_q - alpha * beta_q) / (beta - alpha))\n",
    "\n",
    "        data_q = torch.round(1/s * X + z, decimals=0)\n",
    "        data_q = torch.clip(data_q, alpha_q, beta_q)    \n",
    "        data_q = data_q.to(torch.int8)\n",
    "        \n",
    "        data_qn = data_q\n",
    "        data_qn = data_qn.to(torch.int32)\n",
    "        data_qn = s * (data_qn - z)\n",
    "        data_qn = data_qn.to(torch.float32)\n",
    "    \n",
    "        return data_qn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.batchnorm1(self.conv1(x))\n",
    "        out = self.leaky_relu1(out)\n",
    "        out = self.quantize(out)\n",
    "\n",
    "        out = self.batchnorm2(self.conv2(out))\n",
    "        out = self.leaky_relu2(out)\n",
    "        out = self.quantize(out)\n",
    "        out = self.max_pool2d(out)\n",
    "\n",
    "        out = self.batchnorm3(self.conv3(out))\n",
    "        out = self.leaky_relu3(out)        \n",
    "        out = self.quantize(out)\n",
    "\n",
    "        out = self.batchnorm4(self.conv4(out))\n",
    "        out = self.leaky_relu4(out)\n",
    "        out = self.quantize(out)\n",
    "\n",
    "        out = self.batchnorm5(self.conv5(out))\n",
    "        out = self.leaky_relu5(out)\n",
    "        out = self.quantize(out)\n",
    "        out = self.max_pool2d(out)\n",
    "\n",
    "        out = self.batchnorm6(self.conv6(out))\n",
    "        out = self.leaky_relu6(out)\n",
    "        out = self.quantize(out)\n",
    "\n",
    "        out = self.batchnorm7(self.conv7(out))\n",
    "        out = self.leaky_relu7(out)\n",
    "        out = self.quantize(out)\n",
    "\n",
    "        out = self.batchnorm8(self.conv8(out))\n",
    "        out = self.leaky_relu8(out)\n",
    "        out = self.quantize(out)\n",
    "\n",
    "        out = self.batchnorm9(self.conv9(out))\n",
    "        out = self.leaky_relu9(out)\n",
    "        out = self.quantize(out)\n",
    "        out = self.max_pool2d(out)\n",
    "\n",
    "        #out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.linear_relu(out)\n",
    "        #out = self.softmax(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5991a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR100(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set, batch_size=128,\n",
    "    sampler=test_sampler, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9783de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32bit model cifar100 Accuracy : 0.6901\n"
     ]
    }
   ],
   "source": [
    "model = dongnet12q()\n",
    "model_dict = torch.load('int8qmodel.pth', map_location=torch.device('cpu'))  # 상태 사전 로드\n",
    "model.load_state_dict(model_dict)  # 모델에 상태 사전 로드\n",
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dec705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
