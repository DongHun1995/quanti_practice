{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df51a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from dongnet import dongnet12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c7a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available. Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available! Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228705a",
   "metadata": {},
   "source": [
    "# 모델 선언 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7173299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "batchnorm1.weight\n",
      "batchnorm1.bias\n",
      "conv2.weight\n",
      "batchnorm2.weight\n",
      "batchnorm2.bias\n",
      "conv3.weight\n",
      "batchnorm3.weight\n",
      "batchnorm3.bias\n",
      "conv4.weight\n",
      "batchnorm4.weight\n",
      "batchnorm4.bias\n",
      "conv5.weight\n",
      "batchnorm5.weight\n",
      "batchnorm5.bias\n",
      "conv6.weight\n",
      "batchnorm6.weight\n",
      "batchnorm6.bias\n",
      "conv7.weight\n",
      "batchnorm7.weight\n",
      "batchnorm7.bias\n",
      "conv8.weight\n",
      "batchnorm8.weight\n",
      "batchnorm8.bias\n",
      "conv9.weight\n",
      "batchnorm9.weight\n",
      "batchnorm9.bias\n",
      "linear_relu.0.weight\n",
      "linear_relu.0.bias\n",
      "linear_relu.3.weight\n",
      "linear_relu.3.bias\n",
      "linear_relu.6.weight\n",
      "linear_relu.6.bias\n"
     ]
    }
   ],
   "source": [
    "model = dongnet12()\n",
    "model_dict = torch.load('model.pth', map_location=torch.device('cpu'))  # 상태 사전 로드\n",
    "model.load_state_dict(model_dict)  # 모델에 상태 사전 로드\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1d435",
   "metadata": {},
   "source": [
    "# Quantize 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e67065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(X, NBIT):\n",
    "    # 1. find threshold\n",
    "    alpha = np.max(X)\n",
    "    beta = np.min(X)\n",
    "    alpha_q = -2**(NBIT - 1)\n",
    "    beta_q = 2**(NBIT - 1) - 1\n",
    "\n",
    "    s = (beta - alpha) / (beta_q - alpha_q)\n",
    "    z = int((beta*alpha_q - alpha * beta_q) / (beta - alpha))\n",
    "\n",
    "    data_q = np.round(1/s * X + z, decimals=0)\n",
    "    data_q = np.clip(data_q, alpha_q, beta_q)    \n",
    "    data_q = data_q.astype(np.int8)\n",
    "        \n",
    "    data_qn = data_q\n",
    "    data_qn = data_qn.astype(np.int32)\n",
    "    data_qn = s * (data_qn - z)\n",
    "    data_qn = data_qn.astype(np.float32)\n",
    "    \n",
    "    return data_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a4ce6",
   "metadata": {},
   "source": [
    "# Weight quantize 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f69c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightquantize(NBIT=8):\n",
    "    #quantize시킬 모듈 리스트\n",
    "    quant_module=[nn.Linear, nn.Conv2d, nn.BatchNorm2d]\n",
    "    #quantize시킬 모듈 튜플\n",
    "    quant_module_tuple=tuple(quant_module)\n",
    "    #quntize 모듈에 해당하는 layer 리스트 선언\n",
    "    quant_layer = []\n",
    "\n",
    "    #quantize할 layer 추출\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_module_tuple):\n",
    "            quant_layer.append(name)\n",
    "            \n",
    "    for name, param in model.named_parameters():\n",
    "        convbatchlayer = '.'.join(name.split('.')[:1])\n",
    "        linearlayer = '.'.join(name.split('.')[:2])\n",
    "        if convbatchlayer in quant_layer:\n",
    "            weight = param.cpu().detach().numpy()\n",
    "            dqn = quantize(weight, NBIT)\n",
    "            param.data = torch.from_numpy(dqn)\n",
    "        elif linearlayer in quant_layer:\n",
    "            weight = param.cpu().detach().numpy()\n",
    "            dqn = quantize(weight, NBIT)\n",
    "            param.data = torch.from_numpy(dqn)\n",
    "    \n",
    "    print(\"{}bit quantize Complete\".format(NBIT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14145180",
   "metadata": {},
   "source": [
    "# evaluate 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd739a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, NBIT):\n",
    "    #batch norm 고정, dropout 안함, gradient 계산안함\n",
    "    model.eval()\n",
    "    #model 파라미터를 지정한 device 메모리에 올림\n",
    "    model.to(device)\n",
    "\n",
    "    running_corrects = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #텐서의 최대값과 index를 반환\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # inputs.size(0) 현재 mini-batch의 크기(input의 0번째 dimension 크기)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    print(\"{}bit model cifar100 Accuracy : {:.4f}\".format(NBIT, eval_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d0a75",
   "metadata": {},
   "source": [
    "# test data 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8291d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR100(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set, batch_size=128,\n",
    "    sampler=test_sampler, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66bc52",
   "metadata": {},
   "source": [
    "# 모델 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307cf087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32bit model cifar100 Accuracy : 0.6911\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd722dd0",
   "metadata": {},
   "source": [
    "# quantize 모델 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd39d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8bit quantize Complete\n",
      "8bit model cifar100 Accuracy : 0.0100\n"
     ]
    }
   ],
   "source": [
    "weightquantize(NBIT=8)\n",
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'int8qmodel.pth')\n",
    "weightquantize(NBIT=7)\n",
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=7)\n",
    "weightquantize(NBIT=6)\n",
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=6)\n",
    "weightquantize(NBIT=5)\n",
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=5)\n",
    "weightquantize(NBIT=4)\n",
    "evaluate_model(model=model, test_loader=test_loader, device='cpu', NBIT=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
